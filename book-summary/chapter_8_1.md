# Learning Spark
## chapter 8. 정형화 스트리밍
지금까지 매우 큰 데이터를 처리하는 정형화 api를 사용하는 법을 배웠다.
이번 장에서는 연속적으로 들어오는 데이터를 실시간 형태로 처리될 필요가 있을 때, 정형화 API로 데이터 스트림 처리를 배워보겠다.

### 아파치 스파크의 스트림 처리 엔진 진화
스트림 처리란 연속적으로 들어오는 데이터 흐름을 실시간 처리하는 것이다.
빅데이터의 등장으로 단일 노드 처리 엔진에서 멀티 노드 분산 처리 엔진으로 진화했다.
또한 전통적인 분산 스트림 처리는 레코드 단위 처리 모델로 구현되어 여러 노드에서 입력받은 레코드를 한번에 하나씩 검사해 걸러내고, 다른 노드에서 한번에 하나의 레코드를 처리하는 방식으로 동작한다.
![img](https://file.notion.so/f/s/64f88273-1c96-41f8-87c9-2cb7a68094cf/Untitled.png?id=5e24c49a-c1e4-4d19-a89a-dafbd86f7b18&table=block&spaceId=333f96cf-396d-45ff-8331-232d41bd4d55&expirationTimestamp=1684242806691&signature=F1kC_ceugJBQvkiaa95nELUB3Zn3dIO0Z1kfF-isy5U&downloadName=Untitled.png)
검사 노드와 처리 노드가 분리되어 동작하는 이 모델은 짧은 응답 시간의 장점이 있지만 특정 노드에 장애가 오면 전체적인 작업에 영향이 간다.

### 마이크로 배치 스트림 처리의 출현
> 스트림 처리는 범위가 한정되지 않고(unbounded) 끊임 없이 흘러가는(stream) 데이터에 대한 처리 방식이고, 반대로 한정된(bounded) 데이터의 처리를 배치 처리(batch processing)라고 한다.

작은 배치 처리를 무한히 하는 방식도 스트림 처리에 포함되고, 이를 마이크로 배치(micro-batch)라고 합니다. 
스파크에서는 **마이크로 배치 스트림 처리**라는 새로운 스트림 처리 개념을 제시했다.
스트리밍 데이터 조각들을 아주 작은 맵리듀스 스타일 배치 잡 형태로써 연속적으로 처리하는 모델이다. 

**동작**
끊임없이 들어오는 스트림 데이터를 Spark Streaming이 배치 간격(batch interval)마다 데이터를 나누고, 나눠진 배치 데이터를 Spark Core 엔진이 처리해서 배치 간격마다 결과를 내놓는다. 
즉, 스트림 처리를 작은 시간 간격을 갖는 배치 처리의 연속으로 전환하여 처리한다고 이해하면 된다. 또한 각각의 배치 처리는 Spark으로 분산 처리하기 때문에, 좋은 성능을 가지면서도 장애 복구가 가능한 Spark의 장점들을 모두 지닌다.

**장점**
- 스파크의 테스크 스케줄링은 매우 빠르게 이그제큐터들로 복제해 실행하기 때문에 이그제큐터 장애나 속도 저하에 대응할 수 있다.
- 중복 없는 일회 처리로 신뢰도 있는 결과를 받을 수 있다.
> 스트림 처리의 신뢰도(reliability)에 따른 세 가지 보장 방식
> 1) At-most-once(최대 한 번): 데이터 유실이 있을 수 있어, 추천하지 않는 방식
> 2) At-least-once(적어도 한 번): 데이터 유실은 없으나 재전송으로 인해 중복이 생길 수 있음. 대부분의 경우 충분한 방식
> 3) Exactly-once(딱 한 번): 데이터가 오직 한 번만 처리되어 유실도 중복도 없음. 모든 상황에 대해 완벽히 보장하기 어렵지만 가장 바라는 방식

물론 스트림 처리가 끊임없이 흘러가는 데이터를 처리하다 보니 당연하게도 배치 처리에 비해 데이터 처리 결과를 빠르게 받아볼 수 있다. 하지만 대부분 스트리밍 파이프 라인들은 굳이 초 단위 이하의 반응 속도를 필요로 하지 않으며, 스트림 처리 파이프라인에서는 처리량 달성에서 다른 부분에 더 큰 지연이 발생하는 경우가 많다. 그러므로 배치 처리 방식이 갖는 속도적인 단점은 큰 문제가 되지 않는다.

### 스파크 스트리밍에서 얻은 교훈

### 정형화 스트리밍의 철학

### 정형화 스트리밍의 프로그래밍 모델
정형화 스트리밍에서는 '테이블'의 개념에 착안하여 스트림에서 받는 새로운 레코드를 무한 입력 테이블에 추가되는 새로운 행으로 구조화했다.
그렇게 되면 정형화 스트리밍의 출력 결과는 현 시간까지 입력된 모든 데이터가 정적인 일반 테이블에 저장되어 있는 것과 같고, 그 테이블 위에서 배치 작업이 돌아가는 것과 동일하게 동작된다.


### 정형화 스트리밍 쿼리의 기초 - 5단계

**1단계: 입력 소스 지정**

배치 데이터 소스에서 읽어 들일 때는 DataFrameReader 객체를 만들어주는 spark.read를 썼지만, 스트리밍 소스에 대해서는 DataStreamReader를 만들어주는 spark.readStream을 사용해야 한다.

    - DataStreamReader는 DataFrameReader와 동일한 함수들을 대부분 갖고 있다.
    ```python
    # 파이썬 예제
    spark = SparkSession…

    lines = (spark.readStream.format(“socket”)

    .option(“host”,”localhost”)

    .option(“port”,9999)

    .load())
    ```
    localhost:9999에서 개행 문자로 구분되는 텍스트 데이터를 읽어 무한 테이블 형태의 lines 데이터 프레임을 생성한다.
- spark.read를 쓸 때처럼 스트리밍 데이터를 즉시 읽어 들이는 것은 아니다.
- 위 코드는 실제로 스트리밍 쿼리가 동작할 때 데이터를 읽어들이기 위해 필요한 설정들을 정의하는 것일 뿐이다.
- 소켓 외에도 아파치 스파크는 그 외 DataFrameReader가 지원하는 다양한 파일 기반 포맷(파케이, ORC, JSON 등)에서 데이터 스트림을 읽어들일 수 있게 지원한다.
- 스트리밍 쿼리는 유니언이나 조인같은 데이터 프레임 연산을 써서 조합하는 식으로 다중 입력 소스를 지정할 수도 있다.

**2단계: 데이터 변형**
- counts 변수는 실행 중인 단어 세기 프로그램을 나타내는 ‘스트리밍 데이터 프레임’이며 한번 스트리밍 쿼리가 시작되고 스트리밍 입력이 지속적으로 처리되면서 계산을 수행하게 된다.
- lines  스트리밍 데이터 프레임에 변형을 수행하는 이 연산들은 lines가 일반적인 배치 데이터 프레임이었다 하더라도 정확히 동일한 방식으로 동작한다.
- 일반적으로 배치 데이터 프레임에 쓰이는 대부분의 데이터 프레임은 스트리밍 데이터 프레임에도 적용될 수 있다.


### 실행 중인 스트리밍 쿼리의 내부
### 실행 중인 스트리밍 쿼리의  내부

쿼리가 시작되면 엔진에서는 다음 단계들을 순서대로 실행

- 데이터프레임 연산들은 논리 계획으로 변환됨 → 스파크 SQL이 쿼리 계획을 위한 연산을 추상적으로 표현할 수 있게 함
1. 논리 계획을 분석, 최적화
    - 스파크 SQL이 스트리밍 데이터에 대해 스트리밍 특성에 따라 연속적이고 효과적으로 실행할 수 있는지 확인
2. 스파크 SQL은 아래의 루프를 백그라운드 스레드를 통해 반복 실행
    1. 설정된 트리거 간격마다 스레드는 새 데이터가 있는지 스트리밍되는 인풋 경로(streaming source)를 확인
    2. 새 데이터가 있으면 마이크로 배치로 실행
        - 최적화된 논리 계획으로부터 최적화된 스파크 실행 계획(소스로부터 새로운 데이터를 읽고, 계산을 통해 점진적으로 결과를 업데이트시키고, output에 결과를 작성하는 것까지 포함)이 생성됨
    3. 모든 마이크로 배치마다 특정 분량의 데이터가 처리되고, 관련된 모든 상태가 체크포인트에 저장됨
        - 체크포인트 위치에서 쿼리가 필요할 때마다 정확한 범위를 새로 처리할 수 있게 함
3. 위의 루프는 다음 이유들 중 하나로 질의가 종료되기 전까지 계속
    1. 질의에서 오류 발생 (처리에서 오류가 발생하거나 클러스터 장애 발생)
    2. `streamingQuery.stop()` 호출에 의해 명시적으로 질의 중단
    3. 트리거가 일회 실행(`once`)으로 설정되어 있는 경우
        - 질의는 모든 가능한 데이터를 처리하는 한 번의 마이크로 배치 실행 후 정지
    
    ![스크린샷 2023-05-13 오후 2.57.39.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b8984e80-4860-4a69-9cc9-2c0e05a3e211/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2023-05-13_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.57.39.png)
    
- 정형화 스트리밍의 핵심: 내부적으로 데이터 실행에 스파크 SQL을 사용
    - 스트리밍 처리량을 극대화하기 위해 스파크 SQL의 최적화 실행 엔진의 모든 성능을 끌어다 쓰고, 이것이 성능 효율을 높임

### 정확한 일회 실행을 위한 장애 복구
앞서 살펴보았던 스파크 정형화 스트리밍(마이크로 배치 처리)의 장점은 중복 없는 일회 처리로 신뢰도 있는 결과를 받을 수 있다는 것이었다.
> 스트림 처리의 신뢰도(reliability)에 따른 세 가지 보장 방식
> 1) At-most-once(최대 한 번): 데이터 유실이 있을 수 있어, 추천하지 않는 방식
> 2) At-least-once(적어도 한 번): 데이터 유실은 없으나 재전송으로 인해 중복이 생길 수 있음. 대부분의 경우 충분한 방식
> 3) Exactly-once(정확히 딱 한 번): 데이터가 오직 한 번만 처리되어 유실도 중복도 없음. 모든 상황에 대해 완벽히 보장하기 어렵지만 가장 바라는 방식 보았듯이 스파크 정형화 스트리밍은 Exactly-once(딱 한 번)의 방식을 갖는다.

Exactly-once(정확한 일회 실행) 보장하기 위해서는 전체적으로 아래의 조건들이 만족되어야 한다.
- 재실행 가능한 스트리밍 소스: 지난 마이크로 배치에서 미완료된 데이터 범위를 소스에서 다시 읽을 수 있어야 한다.
- 결정론적 연산: 모든 데이터 변형 결과는 동일한 입력 데이터에 대해 동일해야 한다.
- 멱등성 스트리밍 싱크: 싱크는 재실행된 마이크로 배치를 구분할 수 있어야하고, 재실행으로 중복 쓰기가 발생하면 무시할 수 있어야 한다.

그럼 배치 재실행 및 관련 장애 복구 방안을 살펴보겠다. 

* 종료한 스트리밍 쿼리를 재시작하는 법
    종료한 쿼리를 새 프로세스로 시작하기 위해 새로운 SparkSession을 생성하고 데이터 프레임을 재정의한다.
    처음 쿼리가 시작할 때 사용된 것과 같은 체크포인트 위치로 스트리밍 쿼리를 시작한다.
    > 체크포인트: 스트리밍 애플리케이션의 상태를 주기적으로 저장하여 고장 복구를 도울 수 있는 메커니즘으로 아래 두가지 정보를 저장한다.
    >(1) 메타데이터: 스트리밍 애플리케이션의 설정, DStream 연산, 그리고 작업 진행 상황과 같은 메타데이터가 포함된다. 메타데이터는 애플리케이션 복구를 위해 필요한 정보를 제공한다.
    >(2) 데이터: 스트리밍 애플리케이션에서 사용되는 RDD의 데이터를 저장한다. 이 데이터는 스트리밍 애플리케이션 복구에 필요한 중간 연산 결과를 포함하며, 스트리밍 애플리케이션 복구 시 이전 상태를 복원하는 데 사용된다.

    

**퀴리 재시작 사이 쿼리를 수정하는 법**

    - 데이터 프레임 트랜스포메이션
        작은 수정들을 트랜스포메이션에  추가할 수 있다.
        [개념 복습] Spark Transformation이란 기존의 RDD에서 새로운 RDD를 생성하는 function이다.

        ```python
        from pyspark.sql import *
        from pyspark.sql.types import BooleanType
        from pyspark.sql.functions import *

        ### 잘못된 바이트 배열을 무시하기 위한 udf 함수
        def isCorruptedUdf(value):
            return False

        spark = (SparkSession
                .builder
                .appName("CountWord")
                .getOrCreate())

        spark.udf.register("isCorruptedUdf", isCorruptedUdf, BooleanType())

        lines = (spark
                .readStream.format("socket")
                .option("host", "localhost")
                .option("port", 9999)
                .load())

        # add transformation
        filteredLines = lines.filter("isCorruptedUdf(value) = False")

        # words = lines.select(split(col("value"), "\\s").alias("word"))
        words = filteredLines.select(split(col("value"), "\\s").alias("word"))
        counts = words.groupBy("word").count()
        checkpointDir = "..."

        streamingQuery = (counts
                        .writeStream
                        .format("console")
                        .outputMode("complete")
                        .trigger(processingTime="1 second")
                        .option("checkpointLocation", checkpointDir)
                        .start())

        streamingQuery.awaitTermination()
        ```
        참고 - https://medium.com/@sasidharan-r/how-to-handle-corrupt-or-bad-record-in-apache-spark-custom-logic-pyspark-aws-430ddec9bb41

    - 소스와 싱크 옵션
        읽기 스트림과 쓰기 스트림이 변경될 수 있는지는 싱크와 소스의 종류에 따라 다르다.
        
        데이터를 다른 호스트나 포트로 전송하는 스트림은 중간에 바꾸면 안되고, 반대로 콘솔 수준 싱크의 스트림은 수정 가능하다. 
        
        예를 들어 매 트리거마다 100줄씩 출력하는 옵션을 추가하고 싶다면 아래와 같은 구성을 추가할 수 있다.
        
        ```python
        writeStream.format("console").option("numRows", "100"),,,
        ```
    
    - 상세 부분 처리
        체크포인트 위치는 재시작 사이에 바뀌면 안된다. 
        하지만 트리거의 간격 등의 상세 내용들은 장애 내구성을 보장하는 선에서 변경 가능하다.

    위의 예시 외에도 쿼리 재시작 사이 변경이 허용되는 것들에 대한 자세한 정보들은 이 [가이드](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing)를 참고하면 된다.



## 파일

- 정형화 스트리밍은 배치 처리에서 지원하는 것과 동일한 포맷의 파일들로부터 데이터 스트림을 읽거나 쓰는 것을 지원(ex. 일반 텍스트 파일, CSV, JSON 등)

### 파일 기반의 정형화 스트리밍 처리 방법

1. **파일에서 읽기**
정형화 스트리밍은 디렉터리에 쓰여진 파일들을 하나의 데이터 스트림으로 간주 할 수 있음
    
    ```python
    from pyspark.sql.types import *
    inputDirectoryOfJsonFiles = ...
    fileSchema = (StructType()
     .add(StructField("key", IntegerType()))
     .add(StructField("value", IntegerType())))
    inputDF = (spark
     .readStream
     .format("json")
     .schema(fileSchema)
     .load(inputDirectoryOfJsonFiles))
    ```
    

- **반환된 스트리밍 데이터 프레임은 특정 스키마를 갖게 된다.**

**[파일을 이용할 때 기억할 몇가지 키포인트]**

- 모든 파일들은 동일 포맷이어야 하며, 동일 포맷을 가질 것이라 가정
=> 위 가정이 어긋나면 잘못된 파싱 결과가 나오거나(ex. 모든 값이 null이 되는 등) 쿼리가 실패할 것이다.
- 각각의 파일은 디렉터리에서 완전한 하나의 파일로 존재해야한다.
= 읽는 시점에 전체 파일이 읽기 가능해야하며, 수정되거나 업데이트 되면 안된다.
- 처리해야 할 새로운 파일이 여러 개가 있을 때 처리량 제한 등으로 인해 그중 일부만 처리될 수 있으며 가장 빠른 타임스탬프를 갖고 있는 파일들이 먼저 선택된다.

1. **파일에 쓰기**
- 정형화 스트리밍은 읽기에 쓰이는 동일 포맷 파일에 스트리밍 쿼리 결과를 쓸 수 있다.
- 단, 기존 데이터 파일 수정이 쉽지 않음(업데이트나 전체 모드에서 필요)
- 파일을 새로 추가는 것은 쉬우므로 추가 모드만 지원(데이터를 디렉토리에 추가하는 셈)
```python
#파일에 쓰기 예제
outputDir = ...
checkpointDir = ...
resultDF = ...
streamingQuery = (resultDF.writeStream
 .format("parquet")
 .option("path", outputDir) #"path" 옵션 대신 직접적으로 start(outputDir)에 지정할 수도 있다.
 .option("checkpointLocation", checkpointDir)
 .start())
```

**[기억할 몇 가지 사항]**

- 정형화 스트리밍은 디렉터리에 쓰이는 데이터 파일들의 로그를 유지하며 파일 쓰기 할 때 전체적으로 정확한 일회 처리를 보장. 다른 처리 엔진들은 이런 로그의 존재를 알 수 없으므로 동일하게 보장을 제공하지 못할 수 있다.
- 재시작 사이에 결과 데이터 프레임의 스키라믈 변경한다면, 결과 디렉터리의 파일들이 서로 다른 스키마를 갖고 섞여 있을 수 있다.