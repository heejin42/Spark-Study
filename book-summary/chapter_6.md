# Learning Spark
## chapter 6. 스파크 SQL과 데이터 세트
교재 164p ~ 180p

### 자바와 스칼라를 위한 단일 API
3장에서 나왔듯이 데이터세트는 강력한 형식의 객체를 위해 통합된 단일 API를 제공한다. 
스파크가 지원하는 여러 언어가 있지만 그 중 스칼라와 자바가 강력하게 형식화된 데이터 타입으로 지정된다.

데이터 세트는 도메인별 형식화된 객체로 데이터 프레임 API의 DSL 연산자나 함수형 프로그래밍을 사용할 수 있다.

### 데이터세트를 위한 스칼라 케이스 클래스, 자바빈
스파크는 StringType, BinaryType, IntegerType, BooleanType, MapType과 같은 내부적 데이터 타입을 가지고 있으며, 스칼라 및 자바의 언어별 데이터 타입에 원활하게 매핑된다.

Dataset[T]를 생성하기 위해서는 형식화된 객체를 정의하는 case class가 필요하다. 
예를 들어 아래 형식의 블로거 정보 json 파일을 예시로 들어보자.
```json
{id: 1, first: 'Jules', last: 'Damji', url: 'https://tinyurl.1', date: '1/4/2016', hits: 4535, campaigns: {'twitter', 'linkedin'}}
...
{id: 87, first: 'Brooke', last: 'Wenig', url: 'https://tinyurl.2', date: '5/5/2018', hits: 8908, campaigns: {'twitter', 'linkedin'}}
```

데이터세트를 생성하려면 각 개별 필드를 정의하는 스칼라 케이스 정의가 필요하다. 이는 스키마 역할을 한다.
```sc
case class Blogger(
    id : Int, first: String, last: String, url: String, date: String, hits: Int, campaigns:Array[String])
```
이제 위의 스칼라 케이스를 활용하여 데이터 원본에서 파일을 읽을 수 있다.

```scala
val bloggersDS = spark.read.format("json").option("path", file).load().as[Blogger]
```
이처럼 데이터세트를 만들려면 모든 개별 칼럼의 이름과 유형을 알아야 가능하며 케이스 클래스나 자바빈 클래스가 스키마와 일치해야 한다.
데이터세트 API로 작업하는 것은 데이터 프레임으로 작업하는 것만큼 쉽고 간결하다. 

### 샘플 데이터 변환

- 데이터세트 : 도메인별 객체의 강력하게 정형화된 컬렉션
    - 이러한 객체는 함수적 연산을 사용해 병렬로 변환할 수 있다.
        - `map()`, `reduce()`, `filter()`, `select()`, `aggregate()`

### 데이터세트 및 데이터 프레임을 위한 메모리 관리
스파크는 집중적

### 직렬화 및 역직렬화
- 직렬화: 송신자에 의해 전송 가능한 형태(이진 형식)으로 만들어 객체들의 데이터가 연속적인 데이터로 변환되어 스트림을 통해 데이터를 수신자가 읽을 수 있도록 한다. = 인코딩
- 역직렬화: 수신사에 의해 전송된 스트림 데이터(직렬화된 파일)를 다시 역으로 직렬화하여 객체의 형태로 만든다. = 디코딩
- 스파크 클러스터의 노드 간에 JVM 객체를 공유하는 경우, 바이트 배열로 직렬화되어 송신되며, 수신자가 다시 원래 객체 형태로 복원한다.

기본적으로 JVM에 자체 자바 직렬화기와 역직렬화기가 내장되어 있지만 비효율적이고 느리다. 그에 비해 데이터세트의 인코더는 몇가지 장점이 있다.
1. 자바 힙 메모리에 객체를 저장하며 크기가 작아서 공간을 적게 차지한다. 
2. 메모리 주소와 오프셋이 있는 간단한 포인터 연산으로 메모리를 가로질러 빠르게 직렬화한다.
3. 이진 표현을 스파크 내부 표현으로 빠르게 역직렬화하기 때문에 JVM의 가비지 컬렉션(메모리 관리)로 인한 일시 중지에 영향을 받지 않는다.


